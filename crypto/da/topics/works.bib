@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{an2024does,
  title={Why Does the Effective Context Length of LLMs Fall Short?},
  author={An, Chenxin and Zhang, Jun and Zhong, Ming and Li, Lei and Gong, Shansan and Luo, Yao and Xu, Jingjing and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2410.18745},
  year={2024}
}

@article{fournier2023practical,
  title={A practical survey on faster and lighter transformers},
  author={Fournier, Quentin and Caron, Ga{\'e}tan Marceau and Aloise, Daniel},
  journal={ACM Computing Surveys},
  volume={55},
  number={14s},
  pages={1--40},
  year={2023},
  publisher={ACM New York, NY}
}

@article{dao2023flashattention,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{li2024long,
  title={Long-context llms struggle with long in-context learning},
  author={Li, Tianle and Zhang, Ge and Do, Quy Duc and Yue, Xiang and Chen, Wenhu},
  journal={arXiv preprint arXiv:2404.02060},
  year={2024}
}
@article{behrouz2024titans,
  title={Titans: Learning to memorize at test time},
  author={Behrouz, Ali and Zhong, Peilin and Mirrokni, Vahab},
  journal={arXiv preprint arXiv:2501.00663},
  year={2024}
}
@inproceedings{ulvcar2021training,
  title={Training dataset and dictionary sizes matter in Bert models: the case of Baltic languages},
  author={Ul{\v{c}}ar, Matej and Robnik-{\v{S}}ikonja, Marko},
  booktitle={International Conference on Analysis of Images, Social Networks and Texts},
  pages={162--172},
  year={2021},
  organization={Springer}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{takase2024large,
  title={Large Vocabulary Size Improves Large Language Models},
  author={Takase, Sho and Ri, Ryokan and Kiyono, Shun and Kato, Takuya},
  journal={arXiv preprint arXiv:2406.16508},
  year={2024}
}

@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}

@article{berlyand2021stability,
  title={Stability for the training of deep neural networks and other classifiers},
  author={Berlyand, Leonid and Jabin, Pierre-Emmanuel and Safsten, C Alex},
  journal={Mathematical Models and Methods in Applied Sciences},
  volume={31},
  number={11},
  pages={2345--2390},
  year={2021},
  publisher={World Scientific}
}
